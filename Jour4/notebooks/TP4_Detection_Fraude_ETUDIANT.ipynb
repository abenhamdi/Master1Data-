{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# TP4 - DÉTECTION DE FRAUDE BANCAIRE\n## Optimisation ML & Feature Engineering Avancé\n\n**Master 1 Data Engineering - Concepts & Techno IA** \n**Durée : 7 heures** \n**Nom & Prénom : ___________________________**\n\n---\n\n### Objectifs\n- Maîtriser le Feature Engineering avancé\n- Gérer des données fortement déséquilibrées (0.17% de fraudes)\n- Optimiser des hyperparamètres avec GridSearchCV\n- Construire des Pipelines ML complets\n- Analyser la performance (ROC, Learning Curves, Feature Importance)\n\n### Mission\nDévelopper un modèle de détection de fraude avec :\n- **Recall ≥ 0.85** (détecter 85% des fraudes)\n- **Precision maximale** (minimiser les faux positifs)\n- **Explicabilité** (Feature Importance)\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## IMPORTS & CONFIGURATION"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Imports standards\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Configuration visualisation\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n%matplotlib inline\n\n# Reproductibilité\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\n\nprint(\" Imports réussis\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Imports ML\nfrom sklearn.model_selection import (\n train_test_split, StratifiedKFold, TimeSeriesSplit,\n GridSearchCV, cross_val_score, learning_curve\n)\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.pipeline import Pipeline\n\n# Modèles\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Métriques\nfrom sklearn.metrics import (\n classification_report, confusion_matrix, ConfusionMatrixDisplay,\n roc_auc_score, average_precision_score, precision_recall_curve,\n roc_curve, auc, f1_score, precision_score, recall_score\n)\n\n# Gestion déséquilibre\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\n\nprint(\" Imports ML réussis\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## PARTIE 1 : EXPLORATION & FEATURE ENGINEERING (2h)\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.1 Chargement des Données"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Charger le dataset creditcard.csv depuis le dossier ../data/\n\ndf = None # À compléter\n\n# Afficher les informations\nprint(f\"Shape: {df.shape}\")\ndf.head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Afficher les informations du dataset\n\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.2 Analyse du Déséquilibre"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Calculer et afficher :\n# - Nombre de transactions légitimes (Class = 0)\n# - Nombre de fraudes (Class = 1)\n# - Pourcentage de fraudes\n# - Ratio fraudes/légitimes\n\nfraud_count = None # À compléter\nlegit_count = None # À compléter\nfraud_percentage = None # À compléter\n\nprint(f\"Transactions légitimes: {legit_count:,}\")\nprint(f\"Fraudes: {fraud_count:,}\")\nprint(f\"Pourcentage de fraudes: {fraud_percentage:.3f}%\")\nprint(f\"Ratio (1 fraude pour X légitimes): 1:{legit_count/fraud_count:.0f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Visualiser la distribution des classes avec un bar plot\n\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### QUESTION 1\n**Analysez les résultats ci-dessus et répondez :**\n\n1. Quel est le ratio fraudes/légitimes ? Est-ce un déséquilibre important ?\n2. Pourquoi ce déséquilibre est-il un problème pour le Machine Learning ?\n3. Quelles techniques pouvez-vous utiliser pour gérer ce déséquilibre ?\n\n**Vos réponses :**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```\n1. ...\n\n2. ...\n\n3. ...\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.3 Analyse des Distributions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Comparer les statistiques (mean, std, min, max) pour Amount et Time\n# entre fraudes et transactions légitimes\n\nprint(\"=\"*60)\nprint(\"TRANSACTIONS LÉGITIMES\")\nprint(\"=\"*60)\n# À compléter\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"FRAUDES\")\nprint(\"=\"*60)\n# À compléter"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Créer 2 histogrammes côte à côte pour visualiser la distribution de Amount\n# Un pour les légitimes, un pour les fraudes\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# À compléter\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Créer des boxplots pour comparer Amount entre fraudes et légitimes\n\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.4 Analyse de Corrélation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Calculer la corrélation de toutes les features avec 'Class'\n# Afficher les 10 features les plus corrélées (en valeur absolue)\n\ncorrelations = None # À compléter\n\nprint(\"Top 10 Features corrélées avec Class:\")\n# À compléter"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Créer une heatmap des 10 features les plus corrélées avec Class\n\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### QUESTION 2\n**Analysez les corrélations :**\n\n1. Quelles sont les 3 features PCA les plus corrélées avec Class ?\n2. Les fraudes ont-elles des montants typiques différents des transactions légitimes ?\n3. Y a-t-il des patterns temporels visibles ?\n\n**Vos réponses :**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```\n1. ...\n\n2. ...\n\n3. ...\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.4bis Analyse Temporelle Approfondie"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Analyser la distribution des fraudes par heure\n# Créer un graphique montrant le nombre de fraudes par heure\n# Comparer avec les transactions légitimes\n\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Analyser les patterns jour/nuit\n# Calculer le taux de fraude pour chaque heure\n# Formule: taux_fraude(h) = nb_fraudes(h) / nb_total_transactions(h)\n# Identifier les heures à risque élevé\n\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Créer une matrice de corrélation entre Time, Amount et les top 5 features PCA\n# Visualiser avec une heatmap\n\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n### 1.5 Feature Engineering Avancé\n**Mission : Créer au minimum 15 nouvelles features pertinentes**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Créer une copie pour le feature engineering\ndf_fe = df.copy()\n\nprint(f\"Shape initiale: {df_fe.shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Features Temporelles"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Créer les features temporelles suivantes :\n# 1. 'hour' : Heure de la journée (0-23) à partir de Time\n# 2. 'day' : Jour (0 ou 1) à partir de Time\n# 3. 'hour_sin' : Encodage cyclique sin de l'heure\n# 4. 'hour_cos' : Encodage cyclique cos de l'heure\n\ndf_fe['hour'] = None # À compléter\ndf_fe['day'] = None # À compléter\ndf_fe['hour_sin'] = None # À compléter\ndf_fe['hour_cos'] = None # À compléter\n\nprint(\" Features temporelles créées\")\ndf_fe[['Time', 'hour', 'day', 'hour_sin', 'hour_cos']].head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Créer une feature 'period' (période de la journée)\n# Nuit: 0-6h, Matin: 6-12h, Après-midi: 12-18h, Soir: 18-24h\n\ndef get_period(hour):\n \"\"\"Retourne la période de la journée\"\"\"\n # À compléter\n pass\n\ndf_fe['period'] = None # À compléter\n\nprint(\"Distribution des périodes:\")\nprint(df_fe['period'].value_counts())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Features sur les Montants"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Créer les features suivantes sur Amount :\n# 1. 'amount_log' : Log transformation (gérer l'asymétrie)\n# 2. 'amount_sqrt' : Racine carrée\n# 3. 'is_zero_amount' : Indicateur si montant = 0\n\ndf_fe['amount_log'] = None # À compléter\ndf_fe['amount_sqrt'] = None # À compléter\ndf_fe['is_zero_amount'] = None # À compléter\n\nprint(\" Features sur montants créées\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Créer un binning du montant\n# Catégories suggérées: [0, 10, 50, 100, 500, inf]\n# Labels: ['micro', 'small', 'medium', 'large', 'xlarge']\n\ndf_fe['amount_bin'] = None # À compléter\n\nprint(\"Distribution des bins:\")\nprint(df_fe['amount_bin'].value_counts())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Features d'Interaction"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Créer des features d'interaction\n# Multiplier Amount avec les 3 features PCA les plus corrélées avec Class\n# (identifiées dans l'analyse de corrélation)\n# Exemple: df_fe['V1_amount'] = df_fe['V1'] * df_fe['Amount']\n\n# À compléter (au moins 3 interactions)\n\nprint(\" Features d'interaction créées\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Créer des features d'agrégation sur les V1-V28\n# Exemples:\n# - Somme des 5 premières features PCA\n# - Moyenne des 5 premières features PCA\n# - Écart-type des features PCA\n\n# À compléter\n\nprint(\" Features d'agrégation créées\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Features Polynomiales et Ratios"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Créer des features polynomiales\n# 1. amount_squared = Amount²\n# 2. amount_cubed = Amount³\n# Justification: Capturer les relations non-linéaires\n\ndf_fe['amount_squared'] = None # À compléter\ndf_fe['amount_cubed'] = None # À compléter\n\nprint(\"Features polynomiales créées\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Créer des features de ratio\n# 1. amount_per_hour = Amount / (hour + 1) pour éviter division par 0\n# 2. time_amount_ratio = Time / (Amount + 1)\n# Justification: Relations entre variables temporelles et montants\n\ndf_fe['amount_per_hour'] = None # À compléter\ndf_fe['time_amount_ratio'] = None # À compléter\n\nprint(\"Features de ratio créées\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Créer des features d'écart à la moyenne\n# Pour les 3 features PCA les plus corrélées, calculer:\n# deviation_Vi = |Vi - mean(Vi)| / std(Vi)\n# Justification: Identifier les valeurs anormales (z-score absolu)\n\n# À compléter pour les 3 features les plus corrélées\n\nprint(\"Features d'écart créées\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Validation des Nouvelles Features"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compter les nouvelles features créées\noriginal_features = df.shape[1]\nnew_features_count = df_fe.shape[1] - original_features\n\nprint(f\"Features originales: {original_features}\")\nprint(f\"Nouvelles features créées: {new_features_count}\")\nprint(f\"Total features: {df_fe.shape[1]}\")\n\n# Lister les nouvelles features\nnew_features = [col for col in df_fe.columns if col not in df.columns]\nprint(f\"\\nNouvelles features: {new_features}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Analyser la corrélation des nouvelles features avec Class\n# Afficher les nouvelles features triées par corrélation absolue\n\nprint(\"Corrélation des nouvelles features avec Class:\")\n# À compléter"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### QUESTION 3\n**Justifiez vos choix de Feature Engineering :**\n\n1. Quelles features créées sont les plus prometteuses (corrélation) ?\n2. Pourquoi l'encodage cyclique (sin/cos) est-il pertinent pour l'heure ?\n3. Quel est l'intérêt métier de créer des interactions Amount × V_i ?\n\n**Vos réponses :**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```\n1. ...\n\n2. ...\n\n3. ...\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## PARTIE 2 : MODÉLISATION BASELINE (1h30)\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.1 Préparation des Données"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Séparer X et y\n# Attention: Exclure les colonnes catégorielles non encodées (ex: 'period', 'amount_bin')\n# ou les encoder avant avec pd.get_dummies()\n\n# Encoder les variables catégorielles si nécessaire\n# df_fe_encoded = pd.get_dummies(df_fe, columns=['period', 'amount_bin'], drop_first=True)\n\nX = None # À compléter\ny = None # À compléter\n\nprint(f\"X shape: {X.shape}\")\nprint(f\"y shape: {y.shape}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Split Train/Test\n# Utiliser stratify=y pour préserver le ratio des classes\n# Test size: 20%\n\nX_train, X_test, y_train, y_test = None # À compléter\n\nprint(f\"Train set: {X_train.shape}\")\nprint(f\"Test set: {X_test.shape}\")\nprint(f\"\\nDistribution dans le train:\")\nprint(y_train.value_counts())\nprint(f\"\\nDistribution dans le test:\")\nprint(y_test.value_counts())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Scaling\n# Choisir entre StandardScaler ou RobustScaler\n# RobustScaler est recommandé car il gère mieux les outliers\n\nscaler = None # À compléter\n\n# Fit sur train, transform sur train et test\nX_train_scaled = None # À compléter\nX_test_scaled = None # À compléter\n\nprint(\" Scaling effectué\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### QUESTION 4\n**Justifiez votre choix de scaler :**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```\nJ'ai choisi [StandardScaler/RobustScaler] parce que...\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.2 Modèles Baseline"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Fonction utilitaire pour évaluer un modèle\ndef evaluate_model(model, X_train, X_test, y_train, y_test, model_name=\"Model\"):\n \"\"\"\n Entraîne et évalue un modèle\n \n Returns:\n dict: Dictionnaire avec les métriques\n \"\"\"\n print(f\"\\n{'='*60}\")\n print(f\"ÉVALUATION: {model_name}\")\n print(f\"{'='*60}\")\n \n # TODO: Compléter cette fonction\n # 1. Entraîner le modèle avec fit()\n # 2. Prédire sur le test avec predict() et predict_proba()\n # 3. Calculer les métriques:\n # - Confusion Matrix\n # - Precision, Recall, F1 (precision_score, recall_score, f1_score)\n # - ROC-AUC (roc_auc_score)\n # - PR-AUC (average_precision_score)\n # 4. Afficher les résultats avec print()\n # 5. Retourner un dictionnaire avec les métriques\n \n # À compléter\n \n return {}\n\nprint(\" Fonction d'évaluation créée\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Modèle 1 : Logistic Regression"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Créer et évaluer une Logistic Regression\n# Utiliser class_weight='balanced' pour gérer le déséquilibre\n\nlr_model = None # À compléter\nlr_results = evaluate_model(lr_model, X_train_scaled, X_test_scaled, y_train, y_test, \"Logistic Regression\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Modèle 2 : Decision Tree"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Créer et évaluer un Decision Tree\n# Paramètres suggérés: max_depth=10, class_weight='balanced'\n\ndt_model = None # À compléter\ndt_results = evaluate_model(dt_model, X_train_scaled, X_test_scaled, y_train, y_test, \"Decision Tree\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Modèle 3 : Random Forest"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Créer et évaluer un Random Forest\n# Paramètres suggérés: n_estimators=100, class_weight='balanced'\n\nrf_model = None # À compléter\nrf_results = evaluate_model(rf_model, X_train_scaled, X_test_scaled, y_train, y_test, \"Random Forest\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Modèle 4 : Support Vector Machine (SVM)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Créer et évaluer un SVM\n# Paramètres: kernel='rbf', class_weight='balanced', C=1.0\n# Note: SVM peut être lent sur de grandes données\n\nfrom sklearn.svm import SVC\n\nsvm_model = None # À compléter\nsvm_results = evaluate_model(svm_model, X_train_scaled, X_test_scaled, y_train, y_test, \"SVM\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Modèle 5 : K-Nearest Neighbors (KNN)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Créer et évaluer un KNN\n# Paramètres: n_neighbors=5, weights='distance'\n# Note: KNN nécessite des données scalées\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn_model = None # À compléter\nknn_results = evaluate_model(knn_model, X_train_scaled, X_test_scaled, y_train, y_test, \"KNN\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Modèle 6 : XGBoost (Baseline)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Créer et évaluer un XGBoost baseline\n# Paramètres: n_estimators=100, max_depth=5, learning_rate=0.1\n# scale_pos_weight = nb_negatifs / nb_positifs (pour gérer le déséquilibre)\n\nfrom xgboost import XGBClassifier\n\n# Calculer scale_pos_weight\nscale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\nprint(f\"Scale pos weight: {scale_pos_weight:.2f}\")\n\nxgb_model = None # À compléter\nxgb_results = evaluate_model(xgb_model, X_train_scaled, X_test_scaled, y_train, y_test, \"XGBoost\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.3 Comparaison des Modèles Baseline"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Créer un tableau comparatif des 6 modèles\n# Colonnes: Model, Precision, Recall, F1, ROC-AUC, PR-AUC\n# pd.DataFrame([lr_results, dt_results, rf_results, svm_results, knn_results, xgb_results])\n\ncomparison_df = None # À compléter\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"COMPARAISON DES MODÈLES BASELINE\")\nprint(\"=\"*60)\nprint(comparison_df)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Visualiser la comparaison avec un bar plot\n# Afficher Precision, Recall, F1, PR-AUC pour les 3 modèles\n\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.4 Comparaison des Stratégies de Rééquilibrage"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Comparer class_weight='balanced' vs SMOTE\n# Prendre le meilleur modèle baseline et tester avec SMOTE\n# Formule SMOTE: Génère des exemples synthétiques pour la classe minoritaire\n# Pour chaque exemple minoritaire x_i, choisir k voisins et créer:\n# x_new = x_i + λ * (x_neighbor - x_i) où λ ∈ [0,1]\n\nfrom imblearn.over_sampling import SMOTE\n\n# Appliquer SMOTE\nsmote = SMOTE(random_state=RANDOM_STATE)\nX_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n\nprint(f\"Avant SMOTE: {y_train.value_counts().to_dict()}\")\nprint(f\"Après SMOTE: {pd.Series(y_train_smote).value_counts().to_dict()}\")\n\n# Entraîner le meilleur modèle avec SMOTE\n# À compléter"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Comparer les résultats\n# Créer un tableau: Stratégie | Precision | Recall | F1 | PR-AUC\n# Ligne 1: class_weight='balanced'\n# Ligne 2: SMOTE\n# Analyser quelle stratégie est la plus efficace\n\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### QUESTION 5\n**Analysez les résultats baseline :**\n\n1. Parmi les 6 modèles testés, lequel a le meilleur Recall ? Est-ce suffisant (objectif ≥ 0.85) ?\n2. Pourquoi PR-AUC est-il plus pertinent que ROC-AUC ici ?\n3. Comparez class_weight vs SMOTE. Quelle stratégie est la plus efficace ?\n4. Quel modèle choisiriez-vous pour l'optimisation ? Pourquoi ?\n\n**Vos réponses :**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```\n1. ...\n\n2. ...\n\n3. ...\n\n4. ...\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## PARTIE 3 : OPTIMISATION AVANCÉE (2h)\n---\n\n**Note** : Cette partie peut prendre du temps (GridSearch = 15-30 min). Commencez par une petite grille pour tester !"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.1 Construction d'un Pipeline ML Complet"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Créer un Pipeline intégrant:\n# 1. Scaling (RobustScaler)\n# 2. Modèle (Random Forest - le meilleur baseline)\n\npipeline = None # À compléter\n\nprint(\" Pipeline créé\")\nprint(pipeline)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.2 Optimisation Hyperparamètres - GridSearchCV"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Définir une grille de paramètres pour Random Forest\n# IMPORTANT: Préfixer les paramètres avec 'classifier__' car dans un pipeline\n# Commencez PETIT pour tester (2-3 valeurs par paramètre max)\n# Exemple:\n# param_grid = {\n# 'classifier__n_estimators': [100, 200],\n# 'classifier__max_depth': [10, None],\n# 'classifier__min_samples_split': [2, 5],\n# 'classifier__class_weight': ['balanced']\n# }\n\nparam_grid = {\n # À compléter\n}\n\nn_combinations = np.prod([len(v) for v in param_grid.values()])\nprint(f\"Nombre de combinaisons: {n_combinations}\")\nprint(f\"Avec cv=5 → {n_combinations * 5} entraînements\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Configurer GridSearchCV\n# - cv: StratifiedKFold(n_splits=5)\n# - scoring: 'average_precision' (PR-AUC)\n# - n_jobs: -1 (parallélisation)\n# - verbose: 2 (afficher la progression)\n\ngrid_search = None # À compléter\n\nprint(\" GridSearchCV configuré\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Lancer la recherche (peut prendre plusieurs minutes)\n# Utiliser X_train et y_train (NON scalés, le pipeline s'en occupe)\n\nimport time\nstart_time = time.time()\n\n# À compléter: grid_search.fit(...)\n\nend_time = time.time()\nprint(f\"\\n Temps d'exécution: {(end_time - start_time) / 60:.2f} minutes\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Afficher les résultats de GridSearch\n# - Meilleurs paramètres (grid_search.best_params_)\n# - Meilleur score CV (grid_search.best_score_)\n# - Évaluer sur le test set (grid_search.best_estimator_)\n\nprint(\"=\"*60)\nprint(\"RÉSULTATS GRIDSEARCH\")\nprint(\"=\"*60)\n\n# À compléter"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.3 Optimisation avec RandomizedSearchCV"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Comparer GridSearchCV vs RandomizedSearchCV\n# RandomizedSearchCV teste n_iter combinaisons aléatoires au lieu de toutes\n# Avantage: Plus rapide, peut explorer un espace plus large\n# Formule: Probabilité de trouver le top 5% en testant n combinaisons aléatoires:\n# P = 1 - (0.95)^n\n# Exemple: n=20 → P ≈ 64%, n=60 → P ≈ 95%\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint, uniform\n\n# Définir des distributions de paramètres (plus larges que GridSearch)\nparam_distributions = {\n 'classifier__n_estimators': randint(50, 300),\n 'classifier__max_depth': [5, 10, 15, 20, 25, None],\n 'classifier__min_samples_split': randint(2, 20),\n 'classifier__min_samples_leaf': randint(1, 10),\n 'classifier__max_features': ['sqrt', 'log2', None],\n 'classifier__class_weight': ['balanced']\n}\n\n# Configurer RandomizedSearchCV\nrandom_search = RandomizedSearchCV(\n pipeline,\n param_distributions,\n n_iter=30, # Tester 30 combinaisons aléatoires\n cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE),\n scoring='average_precision',\n n_jobs=-1,\n random_state=RANDOM_STATE,\n verbose=2\n)\n\nprint(\"RandomizedSearchCV configuré\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Lancer RandomizedSearchCV\nimport time\nstart_time = time.time()\n\n# À compléter: random_search.fit(...)\n\nend_time = time.time()\nprint(f\"\\nTemps d'exécution: {(end_time - start_time) / 60:.2f} minutes\")\n\n# Afficher les résultats\nprint(\"\\n\" + \"=\"*60)\nprint(\"RÉSULTATS RANDOMIZEDSEARCH\")\nprint(\"=\"*60)\nprint(f\"Meilleurs paramètres: {random_search.best_params_}\")\nprint(f\"Meilleur score (CV): {random_search.best_score_:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Comparer GridSearch vs RandomizedSearch\n# Créer un tableau comparatif:\n# Méthode | Meilleur Score | Temps (min) | Nb Combinaisons Testées\n\ncomparaison_search = pd.DataFrame({\n 'Méthode': ['GridSearchCV', 'RandomizedSearchCV'],\n 'Meilleur_Score': [None, None], # À compléter\n 'Temps_min': [None, None], # À compléter\n 'Nb_Combinaisons': [None, 30]\n})\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"COMPARAISON DES MÉTHODES D'OPTIMISATION\")\nprint(\"=\"*60)\nprint(comparaison_search)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.4 Optimisation de XGBoost"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Optimiser XGBoost avec RandomizedSearchCV\n# Créer un pipeline avec XGBoost\n# Paramètres à optimiser:\n# - n_estimators: [50, 100, 200, 300]\n# - max_depth: [3, 5, 7, 9]\n# - learning_rate: [0.01, 0.05, 0.1, 0.3]\n# - subsample: [0.6, 0.8, 1.0]\n# - colsample_bytree: [0.6, 0.8, 1.0]\n# - scale_pos_weight: calculé automatiquement\n\npipeline_xgb = Pipeline([\n ('scaler', RobustScaler()),\n ('classifier', XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss'))\n])\n\n# À compléter: définir param_distributions et lancer RandomizedSearchCV\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### QUESTION 6\n**Analysez l'optimisation :**\n\n1. Quels hyperparamètres ont le plus d'impact sur la performance ?\n2. Le gain de performance justifie-t-il le temps de calcul ?\n3. GridSearch vs RandomizedSearch: quelle méthode est la plus efficace ?\n4. XGBoost optimisé vs Random Forest optimisé: lequel est meilleur ?\n5. Atteignez-vous l'objectif de Recall ≥ 0.85 ?\n\n**Vos réponses :**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```\n1. ...\n\n2. ...\n\n3. ...\n\n4. ...\n\n5. ...\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## PARTIE 4 : ANALYSE & DIAGNOSTIC (1h)\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.1 Feature Importance"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Extraire l'importance des features du meilleur modèle\n\nbest_model = None # À compléter\nfeature_importances = None # À compléter\n\n# Créer un DataFrame et afficher le Top 15\n# À compléter"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Visualiser le Top 15 avec un bar plot horizontal\n\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.1bis Permutation Importance"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Calculer la Permutation Importance\n# Principe: Mélanger aléatoirement une feature et mesurer la baisse de performance\n# Formule: PI(f) = Score_original - Score_après_permutation(f)\n# Plus PI est élevé, plus la feature est importante\n\nfrom sklearn.inspection import permutation_importance\n\n# Calculer la permutation importance\nperm_importance = permutation_importance(\n best_model,\n X_test,\n y_test,\n n_repeats=10,\n random_state=RANDOM_STATE,\n scoring='average_precision'\n)\n\n# Créer un DataFrame\nperm_imp_df = pd.DataFrame({\n 'feature': X_train.columns,\n 'importance_mean': perm_importance.importances_mean,\n 'importance_std': perm_importance.importances_std\n}).sort_values('importance_mean', ascending=False)\n\nprint(\"Top 15 Features - Permutation Importance:\")\nprint(perm_imp_df.head(15))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Comparer MDI (Mean Decrease Impurity) vs Permutation Importance\n# Créer un graphique côte à côte\n# MDI peut être biaisé vers les features à haute cardinalité\n# Permutation Importance est plus fiable mais plus coûteux en calcul\n\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.1ter SHAP Values (Explicabilité Avancée)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Calculer les SHAP values\n# SHAP (SHapley Additive exPlanations) basé sur la théorie des jeux\n# Formule de Shapley: φ_i = Σ [|S|!(n-|S|-1)!/n!] × [f(S∪{i}) - f(S)]\n# où S sont tous les sous-ensembles de features sans i\n# Interprétation: Contribution marginale moyenne de chaque feature\n\nimport shap\n\n# Créer l'explainer (TreeExplainer pour Random Forest/XGBoost)\nexplainer = shap.TreeExplainer(best_model.named_steps['classifier'])\n\n# Calculer SHAP values sur un échantillon (100 premières lignes du test)\nX_test_sample = X_test.iloc[:100]\nshap_values = explainer.shap_values(X_test_sample)\n\nprint(\"SHAP values calculés\")\nprint(f\"Shape: {shap_values[1].shape if isinstance(shap_values, list) else shap_values.shape}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Visualiser le summary plot SHAP\n# Ce graphique montre:\n# - L'importance de chaque feature (axe Y)\n# - L'impact sur la prédiction (axe X)\n# - La valeur de la feature (couleur: rouge=élevé, bleu=faible)\n\n# Pour classification binaire, prendre shap_values[1] (classe positive)\nshap_values_plot = shap_values[1] if isinstance(shap_values, list) else shap_values\n\nshap.summary_plot(shap_values_plot, X_test_sample, plot_type=\"dot\", show=False)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Expliquer une prédiction individuelle\n# Choisir une fraude correctement détectée et analyser pourquoi\n# SHAP force plot montre la contribution de chaque feature à la prédiction\n\n# Trouver un exemple de fraude\nfraud_idx = y_test[y_test == 1].index[0]\nfraud_sample_idx = X_test.index.get_loc(fraud_idx)\n\nif fraud_sample_idx < 100: # Si dans notre échantillon\n print(f\"Analyse de la transaction {fraud_idx} (Fraude)\")\n print(f\"Probabilité prédite: {best_model.predict_proba(X_test.iloc[[fraud_sample_idx]])[:,1][0]:.4f}\")\n \n # Force plot\n shap.force_plot(\n explainer.expected_value[1] if isinstance(explainer.expected_value, list) else explainer.expected_value,\n shap_values_plot[fraud_sample_idx],\n X_test_sample.iloc[fraud_sample_idx],\n matplotlib=True,\n show=False\n )\n plt.tight_layout()\n plt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### QUESTION 7\n**Interprétez les Feature Importances :**\n\n1. Les features créées (Feature Engineering) sont-elles utiles ?\n2. Y a-t-il des surprises (features inattendues importantes) ?\n3. MDI vs Permutation Importance: quelles différences observez-vous ?\n4. SHAP: quelles features contribuent le plus aux prédictions de fraude ?\n5. Pourrait-on simplifier le modèle en retirant des features peu importantes ?\n\n**Vos réponses :**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```\n1. ...\n\n2. ...\n\n3. ...\n\n4. ...\n\n5. ...\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.2 Courbes ROC et Precision-Recall"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Tracer les courbes ROC et Precision-Recall côte à côte\n# 1. Prédire les probabilités avec predict_proba()\n# 2. Calculer les courbes avec roc_curve() et precision_recall_curve()\n# 3. Visualiser avec plt.subplot(1, 2, 1) et plt.subplot(1, 2, 2)\n\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.3 Learning Curves"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Tracer les Learning Curves\n# Utiliser learning_curve de sklearn\n# Paramètres: cv=5, scoring='average_precision', train_sizes=np.linspace(0.1, 1.0, 10)\n\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.3bis Calibration des Probabilités"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Vérifier la calibration du modèle\n# Un modèle bien calibré: si proba=0.8, alors 80% des prédictions sont correctes\n# Méthode: Tracer la courbe de calibration (reliability diagram)\n# Axe X: Probabilité prédite (bins)\n# Axe Y: Fraction réelle de positifs dans chaque bin\n# Ligne diagonale = calibration parfaite\n\nfrom sklearn.calibration import calibration_curve\n\n# Calculer la courbe de calibration\nfraction_of_positives, mean_predicted_value = calibration_curve(\n y_test,\n y_proba_optimized,\n n_bins=10,\n strategy='uniform'\n)\n\n# Visualiser\nplt.figure(figsize=(10, 6))\nplt.plot(mean_predicted_value, fraction_of_positives, 's-', label='Modèle')\nplt.plot([0, 1], [0, 1], 'k--', label='Calibration parfaite')\nplt.xlabel('Probabilité prédite moyenne')\nplt.ylabel('Fraction de positifs')\nplt.title('Courbe de Calibration')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"Interprétation:\")\nprint(\"- Si proche de la diagonale: modèle bien calibré\")\nprint(\"- Si au-dessus: modèle sous-estime les probabilités\")\nprint(\"- Si en-dessous: modèle surestime les probabilités\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Appliquer la calibration si nécessaire\n# Méthode: CalibratedClassifierCV avec Platt Scaling ou Isotonic Regression\n# Platt Scaling: Ajuste une régression logistique sur les probabilités\n# Formule: P_calibrated = 1 / (1 + exp(A × log(p/(1-p)) + B))\n# Isotonic Regression: Ajuste une fonction monotone non-paramétrique\n\nfrom sklearn.calibration import CalibratedClassifierCV\n\n# Calibrer le modèle\ncalibrated_model = CalibratedClassifierCV(\n best_model,\n method='sigmoid', # Platt Scaling\n cv=5\n)\n\n# Entraîner sur le train\ncalibrated_model.fit(X_train, y_train)\n\n# Prédire sur le test\ny_proba_calibrated = calibrated_model.predict_proba(X_test)[:, 1]\n\n# Comparer les performances\nprint(\"\\nComparaison avant/après calibration:\")\nprint(f\"PR-AUC avant: {average_precision_score(y_test, y_proba_optimized):.4f}\")\nprint(f\"PR-AUC après: {average_precision_score(y_test, y_proba_calibrated):.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### QUESTION 8\n**Diagnostiquez le modèle :**\n\n1. Le modèle est-il en overfitting, underfitting ou bon fit ?\n2. Le modèle bénéficierait-il de plus de données ?\n3. Quelles actions recommanderiez-vous pour améliorer le modèle ?\n\n**Vos réponses :**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```\n1. ...\n\n2. ...\n\n3. ...\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.4 Optimisation du Seuil de Décision"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Trouver le seuil optimal maximisant le F1-Score\n# 1. Calculer precision/recall pour tous les seuils avec precision_recall_curve()\n# 2. Calculer F1 pour chaque seuil: F1 = 2 * (P * R) / (P + R)\n# 3. Trouver le seuil qui maximise F1\n# 4. Comparer les métriques avec seuil 0.5 vs seuil optimal\n\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### QUESTION 9\n**Analysez l'optimisation du seuil :**\n\n1. Quel est l'impact sur Precision et Recall ?\n2. Quel seuil recommanderiez-vous en production ? Pourquoi ?\n3. Comment choisir entre privilégier Precision ou Recall selon le contexte métier ?\n\n**Vos réponses :**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```\n1. ...\n\n2. ...\n\n3. ...\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## PARTIE 5 : DÉPLOIEMENT & PRODUCTION (1h)\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 5.1 Validation Temporelle (TimeSeriesSplit)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Valider le modèle avec TimeSeriesSplit\n# Comparer avec StratifiedKFold\n\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 5.2 Sérialisation du Modèle"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Sauvegarder le meilleur modèle (pipeline complet)\n\nimport joblib\nimport os\n\n# Créer le dossier models s'il n'existe pas\nos.makedirs('../models', exist_ok=True)\n\n# À compléter"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Sauvegarder les métadonnées\n# Inclure: version, date, paramètres, scores, seuil optimal, features, etc.\n\nmetadata = {\n 'model_version': '1.0',\n 'train_date': '2025-12-14',\n 'random_state': RANDOM_STATE,\n # À compléter\n}\n\n# À compléter: joblib.dump(metadata, ...)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 5.3 Test de Chargement et Prédiction"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Charger le modèle sauvegardé et tester une prédiction\n# 1. Charger avec joblib.load()\n# 2. Prédire sur un échantillon du test\n# 3. Afficher les résultats\n\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 5.4 Création d'une API de Prédiction (Flask)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Créer un script Flask pour l'API\n# Structure:\n# - Endpoint POST /predict\n# - Input: JSON avec les features d'une transaction\n# - Output: JSON avec {\"is_fraud\": bool, \"probability\": float, \"risk_level\": str}\n\n# Créer le fichier api_fraud_detection.py\napi_code = '''\nfrom flask import Flask, request, jsonify\nimport joblib\nimport pandas as pd\nimport numpy as np\n\napp = Flask(__name__)\n\n# Charger le modèle au démarrage\nmodel = joblib.load('models/fraud_detector_v1.joblib')\nmetadata = joblib.load('models/metadata_v1.joblib')\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n \"\"\"\n Endpoint de prédiction\n Input JSON: {\"Time\": float, \"V1\": float, ..., \"Amount\": float}\n Output JSON: {\"is_fraud\": bool, \"probability\": float, \"risk_level\": str}\n \"\"\"\n try:\n # Récupérer les données\n data = request.get_json()\n \n # Convertir en DataFrame\n df = pd.DataFrame([data])\n \n # Prédire\n proba = model.predict_proba(df)[:, 1][0]\n threshold = metadata.get('optimal_threshold', 0.5)\n is_fraud = proba >= threshold\n \n # Déterminer le niveau de risque\n if proba < 0.3:\n risk_level = 'low'\n elif proba < 0.6:\n risk_level = 'medium'\n elif proba < 0.85:\n risk_level = 'high'\n else:\n risk_level = 'critical'\n \n # Retourner la réponse\n return jsonify({\n 'is_fraud': bool(is_fraud),\n 'probability': float(proba),\n 'risk_level': risk_level,\n 'threshold_used': float(threshold)\n })\n \n except Exception as e:\n return jsonify({'error': str(e)}), 400\n\n@app.route('/health', methods=['GET'])\ndef health():\n \"\"\"Endpoint de santé\"\"\"\n return jsonify({'status': 'ok', 'model_version': metadata.get('model_version', 'unknown')})\n\nif __name__ == '__main__':\n app.run(host='0.0.0.0', port=5000, debug=False)\n'''\n\n# Sauvegarder le fichier\nwith open('../api_fraud_detection.py', 'w') as f:\n f.write(api_code)\n\nprint(\"API Flask créée: ../api_fraud_detection.py\")\nprint(\"\\nPour lancer l'API:\")\nprint(\" python api_fraud_detection.py\")\nprint(\"\\nPour tester:\")\nprint(\" curl -X POST http://localhost:5000/predict -H 'Content-Type: application/json' -d '{...}'\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 5.5 Tests Unitaires du Modèle"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Créer des tests unitaires pour valider le modèle\n# Tests à implémenter:\n# 1. Test de chargement du modèle\n# 2. Test de prédiction sur des données valides\n# 3. Test de robustesse (valeurs manquantes, outliers)\n# 4. Test de performance (temps de prédiction < 100ms)\n\nimport unittest\nimport time\n\nclass TestFraudDetector(unittest.TestCase):\n \n @classmethod\n def setUpClass(cls):\n \"\"\"Charger le modèle une fois pour tous les tests\"\"\"\n cls.model = joblib.load('../models/fraud_detector_v1.joblib')\n cls.sample_data = X_test.iloc[:10]\n \n def test_model_loading(self):\n \"\"\"Test: Le modèle se charge correctement\"\"\"\n self.assertIsNotNone(self.model)\n print(\"Test chargement: OK\")\n \n def test_prediction_shape(self):\n \"\"\"Test: Les prédictions ont la bonne forme\"\"\"\n predictions = self.model.predict(self.sample_data)\n self.assertEqual(len(predictions), len(self.sample_data))\n print(\"Test shape prédictions: OK\")\n \n def test_prediction_values(self):\n \"\"\"Test: Les prédictions sont dans [0, 1]\"\"\"\n probas = self.model.predict_proba(self.sample_data)[:, 1]\n self.assertTrue(all(0 <= p <= 1 for p in probas))\n print(\"Test valeurs prédictions: OK\")\n \n def test_prediction_time(self):\n \"\"\"Test: Temps de prédiction < 100ms pour 10 transactions\"\"\"\n start = time.time()\n _ = self.model.predict(self.sample_data)\n elapsed = (time.time() - start) * 1000 # en ms\n self.assertLess(elapsed, 100)\n print(f\"Test temps prédiction: {elapsed:.2f}ms < 100ms OK\")\n\n# Exécuter les tests\nif __name__ == '__main__':\n suite = unittest.TestLoader().loadTestsFromTestCase(TestFraudDetector)\n runner = unittest.TextTestRunner(verbosity=2)\n result = runner.run(suite)\n \n print(f\"\\nTests exécutés: {result.testsRun}\")\n print(f\"Succès: {result.testsRun - len(result.failures) - len(result.errors)}\")\n print(f\"Échecs: {len(result.failures)}\")\n print(f\"Erreurs: {len(result.errors)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## SYNTHÈSE FINALE\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### QUESTION 10 - BILAN GLOBAL\n**Rédigez une synthèse de votre travail (10-15 lignes) :**\n\n1. **Performance finale** : Avez-vous atteint les objectifs (Recall ≥ 0.85, Precision maximale) ?\n2. **Feature Engineering** : Quelles features créées ont été les plus utiles ?\n3. **Optimisation** : Quel a été l'impact de GridSearch et de l'ajustement du seuil ?\n4. **Déploiement** : Le modèle est-il prêt pour la production ? Quelles précautions ?\n5. **Améliorations futures** : Que pourriez-vous faire pour améliorer encore le modèle ?\n\n**Votre synthèse :**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```\n...\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## BONUS (Optionnel)\n---\n\nSi vous avez terminé en avance, voici quelques pistes pour aller plus loin :"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Bonus 1 : XGBoost"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Tester XGBoost avec optimisation des hyperparamètres\n# Paramètre clé: scale_pos_weight (ratio des classes)\n\nfrom xgboost import XGBClassifier\n\n# À compléter"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Bonus 2 : SHAP Values"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Utiliser SHAP pour expliquer les prédictions\n# import shap\n\n# À compléter"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Bonus 3 : Voting Classifier"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Combiner plusieurs modèles avec VotingClassifier\n# from sklearn.ensemble import VotingClassifier\n\n# À compléter"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## FIN DU TP\n\n**Félicitations ! **\n\nVous avez complété un pipeline ML complet de détection de fraude, de l'exploration à la production.\n\n**N'oubliez pas de :**\n- Sauvegarder ce notebook\n- Vérifier que tous les fichiers sont créés (modèle, métadonnées)\n- Répondre à toutes les 10 questions\n- Compléter les modules Python si demandé (`utils/predict.py`)\n\n---\n\n**Master 1 Data Engineering - YNOV Montpellier** \n**Date** : Décembre 2025\n\n---"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}