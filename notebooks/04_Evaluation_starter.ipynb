{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Partie 4 : Évaluation et Métriques\n",
        "\n",
        "Objectifs\n",
        "\n",
        "- Implémenter les métriques de ranking (Precision@K, Recall@K, MAP, NDCG)\n",
        "- Évaluer les trois modèles de recommandation\n",
        "- Comparer les performances\n",
        "- Calculer des métriques business\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.1 - Métriques de Ranking\n",
        "\n",
        "Implémentez les métriques suivantes :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO : Implémenter Precision@K\n",
        "\n",
        "def precision_at_k( ):\n",
        "    \"\"\"\n",
        "    Calcule la Precision@K\n",
        "    \n",
        "    Args:\n",
        "        recommended: liste des items recommandés (ordonnée)\n",
        "        relevant: liste des items pertinents\n",
        "        k: nombre de recommandations à considérer\n",
        "    \n",
        "    Returns:\n",
        "        precision: float entre 0 et 1\n",
        "    \"\"\"\n",
        "    # VOTRE CODE ICI\n",
        "    pass\n",
        "\n",
        "# Test\n",
        "recommended = [1, 3, 5, 7, 9]\n",
        "relevant = [3, 5, 8, 10]\n",
        "\n",
        "print(f\"Precision@5 : {precision_at_k():.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO : Implémenter Recall@K\n",
        "\n",
        "def recall_at_k():\n",
        "    \"\"\"\n",
        "    Calcule le Recall@K\n",
        "    \"\"\"\n",
        "    # VOTRE CODE ICI\n",
        "    pass\n",
        "\n",
        "print(f\"Recall@5 : {recall_at_k(recommended, relevant, 5):.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO : Implémenter MAP (Mean Average Precision)\n",
        "\n",
        "def average_precision(recommended, relevant):\n",
        "    \"\"\"\n",
        "    Calcule l'Average Precision pour un utilisateur\n",
        "    \"\"\"\n",
        "    # VOTRE CODE ICI\n",
        "    pass\n",
        "\n",
        "def mean_average_precision(all_recommended, all_relevant):\n",
        "    \"\"\"\n",
        "    Calcule la MAP sur tous les utilisateurs\n",
        "    \"\"\"\n",
        "    # VOTRE CODE ICI\n",
        "    pass\n",
        "\n",
        "print(f\"Average Precision : {average_precision(recommended, relevant):.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO : Implémenter NDCG (Normalized Discounted Cumulative Gain)\n",
        "\n",
        "def dcg_at_k(relevances, k):\n",
        "    \"\"\"\n",
        "    Calcule le DCG@K\n",
        "    \"\"\"\n",
        "    # VOTRE CODE ICI\n",
        "    pass\n",
        "\n",
        "def ndcg_at_k(recommended, relevant, k):\n",
        "    \"\"\"\n",
        "    Calcule le NDCG@K\n",
        "    \"\"\"\n",
        "    # VOTRE CODE ICI\n",
        "    pass\n",
        "\n",
        "print(f\"NDCG@5 : {ndcg_at_k(recommended, relevant, 5):.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.2 - Préparation du Test Set\n",
        "\n",
        "Créez un test set temporel pour évaluer les modèles.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO : Charger les données\n",
        "DATA_PATH = \"../data/\"\n",
        "interactions_df = pd.read_csv(DATA_PATH + 'interactions.csv')\n",
        "interactions_df['interaction_date'] = pd.to_datetime(interactions_df['interaction_date'])\n",
        "\n",
        "# TODO : Split temporel (80% train, 20% test)\n",
        "# VOTRE CODE ICI\n",
        "\n",
        "print(f\"Train : {len(train_interactions)} interactions\")\n",
        "print(f\"Test : {len(test_interactions)} interactions\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO : Fonction pour obtenir les produits pertinents\n",
        "\n",
        "def get_relevant_products(user_id, test_interactions):\n",
        "    \"\"\"\n",
        "    Retourne les produits pertinents pour un utilisateur dans le test set\n",
        "    \"\"\"\n",
        "    # VOTRE CODE ICI\n",
        "    pass\n",
        "\n",
        "# Test\n",
        "test_users = test_interactions['user_id'].unique()\n",
        "print(f\"Nombre d'utilisateurs à tester : {len(test_users)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.3 - Évaluation des Modèles\n",
        "\n",
        "Évaluez chaque modèle avec les métriques implémentées.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO : Fonction d'évaluation générique\n",
        "\n",
        "def evaluate_model(recommend_func, test_users, test_interactions, k=10):\n",
        "    \"\"\"\n",
        "    Évalue un modèle de recommandation\n",
        "    \n",
        "    Args:\n",
        "        recommend_func: fonction qui prend user_id et retourne liste de recommendations\n",
        "        test_users: liste des utilisateurs à tester\n",
        "        test_interactions: DataFrame des interactions de test\n",
        "        k: nombre de recommandations\n",
        "    \n",
        "    Returns:\n",
        "        dict: dictionnaire des métriques\n",
        "    \"\"\"\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    aps = []\n",
        "    ndcgs = []\n",
        "    \n",
        "    # VOTRE CODE ICI\n",
        "    # Pour chaque utilisateur :\n",
        "    #   - Obtenir les produits pertinents (ground truth)\n",
        "    #   - Obtenir les recommandations du modèle\n",
        "    #   - Calculer les métriques\n",
        "    \n",
        "    return {\n",
        "        'Precision@K': np.mean(precisions),\n",
        "        'Recall@K': np.mean(recalls),\n",
        "        'MAP': np.mean(aps),\n",
        "        'NDCG@K': np.mean(ndcgs)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO : Évaluer chaque modèle\n",
        "\n",
        "# Charger vos modèles du notebook précédent ou les recréer\n",
        "\n",
        "print(\"Évaluation Collaborative Filtering...\")\n",
        "metrics_cf = # VOTRE CODE ICI\n",
        "\n",
        "print(\"Évaluation Content-Based...\")\n",
        "metrics_content = # VOTRE CODE ICI\n",
        "\n",
        "print(\"Évaluation Modèle Hybride...\")\n",
        "metrics_hybrid = # VOTRE CODE ICI\n",
        "\n",
        "print(\"Évaluation terminée\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.4 - Comparaison des Modèles\n",
        "\n",
        "\n",
        "Comparez les performances des trois modèles.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO : Créer un tableau comparatif\n",
        "\n",
        "results = pd.DataFrame({\n",
        "    'Collaborative Filtering': metrics_cf,\n",
        "    'Content-Based': metrics_content,\n",
        "    'Modèle Hybride': metrics_hybrid\n",
        "}).T\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"  COMPARAISON DES MODÈLES\")\n",
        "print(\"=\"*60)\n",
        "print(results.round(3))\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO : Visualisation comparative\n",
        "\n",
        "results.plot(kind='bar', figsize=(12, 6))\n",
        "plt.title('Comparaison des Performances des Modèles')\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Modèle')\n",
        "plt.legend(title='Métriques')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.5 - Métriques Business\n",
        "\n",
        "Calculez des métriques orientées métier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO : Couverture du catalogue\n",
        "\n",
        "def catalog_coverage(all_recommendations, total_products):\n",
        "    \"\"\"\n",
        "    Pourcentage de produits recommandés au moins une fois\n",
        "    \"\"\"\n",
        "    # VOTRE CODE ICI\n",
        "    pass\n",
        "\n",
        "# TODO : Diversité\n",
        "\n",
        "def diversity_score(recommendations, products_df):\n",
        "    \"\"\"\n",
        "    Nombre moyen de catégories différentes dans les recommandations\n",
        "    \"\"\"\n",
        "    # VOTRE CODE ICI\n",
        "    pass\n",
        "\n",
        "# TODO : Nouveauté\n",
        "\n",
        "def novelty_score(recommendations, product_popularity):\n",
        "    \"\"\"\n",
        "    Score de nouveauté basé sur la popularité inversée\n",
        "    \"\"\"\n",
        "    # VOTRE CODE ICI\n",
        "    pass\n",
        "\n",
        "# Calculer pour chaque modèle\n",
        "# VOTRE CODE ICI\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.6 - Analyse et Conclusions\n",
        "\n",
        "Rédigez vos observations et conclusions :\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vos Conclusions\n",
        "\n",
        "Répondez aux questions suivantes :\n",
        "\n",
        "1. Quel modèle obtient les meilleures performances globales ?\n",
        "\n",
        "2. Quels sont les forces et faiblesses de chaque approche ?\n",
        "\n",
        "3. Dans quel contexte recommanderiez-vous chaque modèle ?\n",
        "\n",
        "4. Quelles améliorations proposez-vous ?\n",
        "\n",
        "5. Comment gérez-vous le problème du cold start ?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Vous avez terminé le TP principal. Si vous avez du temps, explorez les bonus dans le sujet (MLOps, API, Deep Learning, etc.)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
